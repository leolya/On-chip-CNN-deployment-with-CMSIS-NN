{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input, ReLU, Flatten, Dense, Activation, Dropout, Softmax, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "quantize numpy array\n",
    "return:\n",
    "q_x: quantized array\n",
    "fp_x: reverse quantized array to float\n",
    "\"\"\"\n",
    "def quantize_array(x, bit_depth=16):\n",
    "    min_x = x.min() \n",
    "    max_x = x.max()\n",
    "\n",
    "    #find number of integer bits to represent this range\n",
    "    int_bits = int(np.ceil(np.log2(max(abs(min_x),abs(max_x)))))\n",
    "    \n",
    "    if int_bits < 0: int_bits = 0\n",
    "\n",
    "    frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "\n",
    "    #floating point weights are scaled and rounded to [-128,127], which are used in \n",
    "    #the fixed-point operations on the actual hardware (i.e., microcontroller)\n",
    "    q_x = np.round(x*(2**frac_bits))\n",
    "\n",
    "    #To quantify the impact of quantized weights, scale them back to\n",
    "    # original range to run inference using quantized weights\n",
    "    fp_x = q_x/(2**frac_bits)\n",
    "    \n",
    "    return q_x, fp_x, int_bits, frac_bits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "compute frac_bits based on max and min list\n",
    "used for activation quantization\n",
    "\"\"\"\n",
    "def min_max_quantize(max_list, min_list, bit_depth=16):\n",
    "    frac_list = []\n",
    "    for i in range(len(max_list)):\n",
    "        int_bits = int(np.ceil(np.log2(max(abs(min_list[i]),abs(max_list[i])))))\n",
    "        if int_bits < 0: int_bits = 0\n",
    "#         if int_bits > bit_depth-1: int_bits = bit_depth-1    \n",
    "        \n",
    "        frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "        frac_list.append(frac_bits)\n",
    "    return frac_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original model\n",
    "def CNN():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    \n",
    "    conv1 = Conv2D(16, 3, padding='same', name='conv1')(inputs)\n",
    "    conv1 = ReLU()(conv1)\n",
    "    \n",
    "    pool1 = MaxPool2D(2)(conv1)\n",
    "        \n",
    "    conv2 = Conv2D(32, 3, padding='same', name='conv2')(pool1)\n",
    "    conv2 = ReLU()(conv2)\n",
    "    \n",
    "    pool2 = MaxPool2D(2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(64, 3, padding='same', name='conv3')(pool2)\n",
    "    conv3 = ReLU()(conv3)\n",
    "    \n",
    "    pool3 = GlobalAveragePooling2D()(conv3)\n",
    "    \n",
    "    fc1 = Flatten()(pool3)\n",
    "    fc1 = Dense(64)(fc1)\n",
    "    fc1 = ReLU()(fc1)\n",
    "    \n",
    "    fc2 = Dense(10)(fc1)\n",
    "    outputs =Softmax()(fc2)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# difine a new model with shifting\n",
    "def CNN_Q(shift=True, bit=16, shift_list=[0, 0, 0, 0, 0]):\n",
    "    \n",
    "    if shift:\n",
    "        assert len(shift_list) == 5\n",
    "    \n",
    "    inputs = Input(shape=(28,28,1), name='input')\n",
    "    \n",
    "    x = Conv2D(16, 3, padding='same', name='conv1')(inputs)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[0]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(2)(x)\n",
    "        \n",
    "    x = Conv2D(32, 3, padding='same', name='conv2')(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[1]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(2)(x)\n",
    "    \n",
    "    x = Conv2D(64, 3, padding='same', name='conv3')(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[2]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, name='fc1')(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[3]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(10, name='fc2')(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[4]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    \n",
    "    if shift:\n",
    "        outputs = x\n",
    "    else:\n",
    "        outputs =Softmax()(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def evaluation(model, inputs, target, input_shift=None):\n",
    "    num = target.shape[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    if input_shift is not None:\n",
    "        inputs = np.round(inputs*(2**input_shift))\n",
    "        \n",
    "    for i in range(num):\n",
    "        predict = np.argmax(model(inputs[i:i+1])[0])\n",
    "        label = np.argmax(target[i])\n",
    "        if predict == label:\n",
    "            cnt += 1\n",
    "    acc = cnt / num\n",
    "    print(\"accuracy: \", acc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 28,106\n",
      "Trainable params: 28,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy:  0.9912\n"
     ]
    }
   ],
   "source": [
    "x_test = np.load('./dataset/x_test.npy')\n",
    "y_test = np.load('./dataset/y_test.npy')\n",
    "x_test = 2*x_test - 1\n",
    "\n",
    "model = CNN_Q(shift=False)\n",
    "model.summary()\n",
    "model.load_weights(\"./model/model.h5\")\n",
    "evaluation(model, x_test, y_test)\n",
    "\n",
    "# num = y_test.shape[0]\n",
    "# cnt = 0\n",
    "# for i in range(num):\n",
    "#     predict = np.argmax(model(x_test[i:i+1])[0])\n",
    "#     label = np.argmax(y_test[i])\n",
    "#     if predict == label:\n",
    "#         cnt += 1\n",
    "# acc = cnt / num\n",
    "# print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n"
     ]
    }
   ],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    if len(layer.get_weights()) != 0:\n",
    "        layer_names.append(layer.name)\n",
    "print(layer_names)\n",
    "layer_num = len(layer_names)\n",
    "\n",
    "intermediate_output = [model.get_layer(\"input\").output]\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(model.get_layer(layer_names[i]).output)\n",
    "\n",
    "intermediate_layer_model = Model(inputs = model.input, outputs = intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = x_test.shape[0]\n",
    "\n",
    "max_list = []\n",
    "min_list = []\n",
    "\n",
    "for i in range(length):\n",
    "    pred = intermediate_layer_model(x_test[i: i+1])\n",
    "    for j in range(len(pred)):\n",
    "        pred_np = pred[j].numpy()\n",
    "        max_np = pred_np.max()\n",
    "        min_np = pred_np.min()\n",
    "        if len(max_list) != len(pred):\n",
    "            max_list.append(max_np)\n",
    "            min_list.append(min_np)\n",
    "        else:\n",
    "            if max_np > max_list[j]:\n",
    "                max_list[j] = max_np\n",
    "            if min_np < min_list[j]:\n",
    "                min_list[j] = min_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  15\n",
      "conv1 :   12\n",
      "conv2 :   9\n",
      "conv3 :   7\n",
      "fc1 :   11\n",
      "fc2 :   9\n"
     ]
    }
   ],
   "source": [
    "frac_bits_activation_list = min_max_quantize(max_list, min_list, bit_depth=16)\n",
    "\n",
    "print(\"input: \", frac_bits_activation_list[0])\n",
    "for i in range(layer_num):\n",
    "    print(layer_names[i], \":  \", frac_bits_activation_list[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "conv1 - - weight - - Q0.15\n",
      "conv1 - - bias - - Q0.15\n",
      "**********\n",
      "conv2 - - weight - - Q1.14\n",
      "conv2 - - bias - - Q0.15\n",
      "**********\n",
      "conv3 - - weight - - Q1.14\n",
      "conv3 - - bias - - Q0.15\n",
      "**********\n",
      "fc1 - - weight - - Q1.14\n",
      "fc1 - - bias - - Q0.15\n",
      "**********\n",
      "fc2 - - weight - - Q1.14\n",
      "fc2 - - bias - - Q0.15\n"
     ]
    }
   ],
   "source": [
    "bit_depth = 16\n",
    "frac_bits_weight_list = []\n",
    "frac_bits_bias_list = []\n",
    "q_weight_list = []\n",
    "q_bias_list = []\n",
    "\n",
    "for i in range(layer_num):\n",
    "    weight, bias = model.get_layer(layer_names[i]).get_weights()\n",
    "    q_weight, f_weight, int_bits_weight, frac_bits_weight = quantize_array(weight, bit_depth=bit_depth)\n",
    "    q_bias, f_bias, int_bits_bias, frac_bits_bias = quantize_array(bias, bit_depth=bit_depth)\n",
    "    \n",
    "    q_weight_list.append(q_weight)\n",
    "    q_bias_list.append(q_bias)\n",
    "    frac_bits_weight_list.append(frac_bits_weight)\n",
    "    frac_bits_bias_list.append(frac_bits_bias)\n",
    "    print(\"**********\")\n",
    "    print(layer_names[i] + \" - - weight - - Q\"+str(int_bits_weight)+\".\"+str(frac_bits_weight))\n",
    "    print(layer_names[i] + \" - - bias - - Q\"+str(int_bits_bias)+\".\"+str(frac_bits_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 11  8  6 10]\n",
      "[18 17 16 10 16]\n"
     ]
    }
   ],
   "source": [
    "bias_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_bias_list)\n",
    "output_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_activation_list[1:])\n",
    "print(bias_shift_list)\n",
    "print(output_shift_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = CNN_Q(shift_list=output_shift_list)\n",
    "\n",
    "for i in range(layer_num):\n",
    "    q_model.get_layer(layer_names[i]).set_weights([q_weight_list[i], q_bias_list[i]*(2**bias_shift_list[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9912\n"
     ]
    }
   ],
   "source": [
    "evaluation(q_model, x_test, y_test, input_shift=frac_bits_activation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[  3805.,  -5958., -19357., -25462.,  17702.,  -4173.,\n",
       "            -7702.,  22197.,   5664., -10594.,   6790.,  -8613.,\n",
       "            -5282.,  -8296.,  15614.,    181.]],\n",
       " \n",
       "         [[ -4213.,  14597.,  -4920.,  12246.,  10889., -26768.,\n",
       "            13048.,  18280.,   8663.,  -8045.,  -8811.,    445.,\n",
       "            -5404.,  -8906.,  16863.,  15067.]],\n",
       " \n",
       "         [[-23831.,   7545.,  22822.,  10542.,   9010., -20788.,\n",
       "             3047.,  11497.,   6381.,  -6576., -12929.,  15176.,\n",
       "            -3646.,  -8591.,  11336.,  17138.]]],\n",
       " \n",
       " \n",
       "        [[[  9568., -16465.,  -7097.,  12701.,  -7625.,  25360.,\n",
       "           -20016.,  17745.,  15052.,   -227.,   9371.,  -5114.,\n",
       "            -4341.,  -2947.,   6308.,   1838.]],\n",
       " \n",
       "         [[ 22940.,  18937.,   1137.,  20921.,  -6855.,  -1374.,\n",
       "             3727.,  17453.,  17579.,  -5755.,    697.,  -6362.,\n",
       "             -881.,  -2954.,  10200., -13213.]],\n",
       " \n",
       "         [[  3554.,  -6191.,  14487.,  -9557.,  -7431.,  -1407.,\n",
       "            14149.,  -1881.,  13122.,  -2092., -14443.,   7532.,\n",
       "             5315.,  -9480.,  20129., -16607.]]],\n",
       " \n",
       " \n",
       "        [[[-16052.,   1733.,   4178.,  15027.,  -7271.,  12017.,\n",
       "            -9221.,  19735., -24021.,   9584.,  13784.,  -9823.,\n",
       "            -4869.,  -5509.,  -5697.,   -884.]],\n",
       " \n",
       "         [[  7086.,  20895.,  12309.,   4332.,  -6480.,  14156.,\n",
       "             1673.,   5465., -26740.,  11803.,   7296.,  -9560.,\n",
       "             9515.,  -8706.,   3127.,  -2816.]],\n",
       " \n",
       "         [[  4424.,  -1796.,   7658., -29017.,  -4163.,  11817.,\n",
       "             5913., -28805.,  -6729.,   4470.,   -934.,  18558.,\n",
       "             4000.,   -493.,  24428.,   1684.]]]], dtype=float32),\n",
       " array([ 1.1141120e+06,  2.7603763e+08,  2.3491379e+08,  2.4667750e+08,\n",
       "        -1.3359514e+08,  2.1276262e+08,  1.1026432e+08,  3.9141376e+08,\n",
       "         1.3887078e+08, -1.3955891e+08, -3.5061760e+06, -6.4028672e+07,\n",
       "        -1.3533184e+08,  2.1626880e+06,  2.8927590e+08, -4.9184768e+07],\n",
       "       dtype=float32),\n",
       " array([[[[-2.6040e+03,  3.0190e+03,  3.1750e+03, ...,  6.0960e+03,\n",
       "            3.0400e+03, -3.7140e+03],\n",
       "          [-2.1140e+03,  3.6700e+02,  2.8980e+03, ..., -1.6100e+03,\n",
       "           -4.0000e+01, -1.0354e+04],\n",
       "          [-2.7850e+03, -9.8000e+01,  1.7730e+03, ..., -2.8820e+03,\n",
       "           -3.7570e+03, -6.3910e+03],\n",
       "          ...,\n",
       "          [ 8.4500e+02, -5.0510e+03, -4.1780e+03, ...,  6.9900e+02,\n",
       "            6.7800e+02, -3.0600e+02],\n",
       "          [-1.1580e+03,  1.4280e+03,  1.0610e+03, ...,  5.9100e+02,\n",
       "           -3.3120e+03, -4.0620e+03],\n",
       "          [ 2.2580e+03, -1.4910e+03,  2.6290e+03, ...,  7.5110e+03,\n",
       "            7.3630e+03, -1.2340e+03]],\n",
       " \n",
       "         [[ 2.9300e+02, -2.1010e+03, -2.3360e+03, ...,  4.5120e+03,\n",
       "           -1.5590e+03, -4.9460e+03],\n",
       "          [ 5.5430e+03,  2.4250e+03,  1.5400e+02, ...,  1.2700e+03,\n",
       "            1.6940e+03, -1.3481e+04],\n",
       "          [ 3.3340e+03,  2.8960e+03,  1.5750e+03, ...,  8.9700e+02,\n",
       "            4.7210e+03, -7.6450e+03],\n",
       "          ...,\n",
       "          [-2.5240e+03, -2.5410e+03, -7.4220e+03, ..., -1.0660e+03,\n",
       "           -2.5500e+03, -1.1590e+03],\n",
       "          [ 2.6500e+02,  2.7650e+03, -1.4950e+03, ...,  3.7300e+02,\n",
       "           -7.5400e+02, -3.1640e+03],\n",
       "          [ 2.4500e+02, -1.2370e+03,  3.7190e+03, ..., -8.0900e+02,\n",
       "           -1.3300e+03,  5.1920e+03]],\n",
       " \n",
       "         [[ 8.6000e+01,  9.6000e+02,  5.9480e+03, ..., -3.8800e+02,\n",
       "           -1.9100e+03,  3.7660e+03],\n",
       "          [ 9.2500e+02,  3.6070e+03,  1.3830e+03, ...,  4.7480e+03,\n",
       "            2.7960e+03, -1.0216e+04],\n",
       "          [ 1.2260e+03,  1.5060e+03,  4.1700e+02, ...,  5.4410e+03,\n",
       "            4.0000e+03, -6.5560e+03],\n",
       "          ...,\n",
       "          [-1.8060e+03, -8.0150e+03, -4.4790e+03, ..., -1.8380e+03,\n",
       "           -4.7600e+02, -5.0800e+02],\n",
       "          [-4.7200e+02,  6.3980e+03, -5.0100e+02, ...,  3.2780e+03,\n",
       "           -1.4200e+03, -3.2350e+03],\n",
       "          [-1.6670e+03,  1.7000e+03,  2.5610e+03, ...,  7.5900e+02,\n",
       "            4.7000e+01,  4.6750e+03]]],\n",
       " \n",
       " \n",
       "        [[[-5.1900e+02,  1.7010e+03,  4.2690e+03, ..., -1.9640e+03,\n",
       "           -1.3530e+03, -6.6010e+03],\n",
       "          [-2.7030e+03,  1.3760e+03,  6.4600e+02, ..., -5.7490e+03,\n",
       "            4.5090e+03, -8.8770e+03],\n",
       "          [-2.9620e+03, -2.8070e+03,  9.6800e+02, ..., -1.1730e+03,\n",
       "            5.7310e+03, -3.4980e+03],\n",
       "          ...,\n",
       "          [-7.5900e+02,  2.9800e+02, -8.6110e+03, ...,  2.8800e+02,\n",
       "           -1.8980e+03,  1.2540e+03],\n",
       "          [-2.6260e+03,  2.7060e+03,  1.3130e+03, ..., -2.8690e+03,\n",
       "           -3.7600e+03, -1.4744e+04],\n",
       "          [-1.7620e+03,  2.2540e+03, -8.4600e+02, ...,  1.6200e+02,\n",
       "           -1.2680e+03,  5.0280e+03]],\n",
       " \n",
       "         [[-1.1960e+03, -2.1300e+02,  3.4170e+03, ...,  3.2680e+03,\n",
       "           -4.6860e+03,  1.5970e+03],\n",
       "          [ 3.3100e+03, -6.9000e+01, -3.1280e+03, ..., -4.9760e+03,\n",
       "            2.7240e+03, -5.1370e+03],\n",
       "          [ 4.3290e+03, -1.2340e+03, -6.5300e+02, ..., -3.3930e+03,\n",
       "            4.2290e+03, -1.8890e+03],\n",
       "          ...,\n",
       "          [-1.6760e+03,  2.1400e+02, -7.0740e+03, ..., -2.2490e+03,\n",
       "           -5.4950e+03,  9.9000e+01],\n",
       "          [ 2.3250e+03,  5.7800e+02, -6.0000e+00, ...,  3.8570e+03,\n",
       "           -1.4500e+02, -4.3130e+03],\n",
       "          [-9.3200e+02, -7.7900e+02,  3.3140e+03, ...,  1.9450e+03,\n",
       "            9.3900e+02,  5.9300e+03]],\n",
       " \n",
       "         [[-4.7700e+02, -3.7970e+03,  5.2250e+03, ..., -4.0130e+03,\n",
       "           -3.6920e+03,  7.0410e+03],\n",
       "          [ 7.7000e+02,  9.7700e+02,  2.9400e+03, ...,  1.1030e+03,\n",
       "            2.0820e+03,  2.2620e+03],\n",
       "          [-2.8400e+02, -3.2130e+03,  1.6600e+02, ...,  1.0680e+03,\n",
       "           -4.9510e+03,  4.8040e+03],\n",
       "          ...,\n",
       "          [-4.1800e+02,  7.9300e+02, -3.5090e+03, ..., -4.9800e+02,\n",
       "            1.1020e+03,  6.2100e+02],\n",
       "          [-7.7400e+02,  2.6370e+03, -1.7900e+02, ...,  5.3320e+03,\n",
       "           -2.7170e+03,  3.4860e+03],\n",
       "          [-2.6940e+03,  1.8070e+03,  5.2230e+03, ...,  9.1300e+02,\n",
       "            7.0600e+02,  3.7360e+03]]],\n",
       " \n",
       " \n",
       "        [[[ 3.3890e+03, -3.0150e+03,  8.8000e+03, ...,  5.9290e+03,\n",
       "           -6.5960e+03,  2.6900e+03],\n",
       "          [ 4.0800e+02,  3.3420e+03,  2.9210e+03, ..., -9.5500e+03,\n",
       "            2.6110e+03,  5.1410e+03],\n",
       "          [-1.0150e+03,  5.0580e+03,  1.5230e+03, ..., -5.0140e+03,\n",
       "            4.1620e+03,  1.0132e+04],\n",
       "          ...,\n",
       "          [-1.8900e+02,  1.8600e+02, -8.7230e+03, ...,  1.0470e+03,\n",
       "           -2.6240e+03, -3.3870e+03],\n",
       "          [-1.3680e+03,  3.0470e+03,  3.4760e+03, ..., -6.9110e+03,\n",
       "           -1.7400e+02, -3.2350e+03],\n",
       "          [-3.0760e+03, -4.7130e+03,  4.7870e+03, ..., -6.9270e+03,\n",
       "            3.6120e+03, -6.0840e+03]],\n",
       " \n",
       "         [[-2.2420e+03,  1.7100e+02,  3.6990e+03, ...,  2.7300e+03,\n",
       "           -1.3890e+03, -2.5390e+03],\n",
       "          [ 3.5910e+03,  2.4690e+03,  1.2770e+03, ..., -4.4020e+03,\n",
       "           -6.9200e+02,  6.5980e+03],\n",
       "          [ 4.4850e+03,  1.4650e+03,  1.2410e+03, ..., -3.2400e+03,\n",
       "           -3.2700e+02,  6.3460e+03],\n",
       "          ...,\n",
       "          [-2.4310e+03, -1.5700e+02, -4.1100e+03, ...,  8.8200e+02,\n",
       "           -2.5130e+03, -4.0880e+03],\n",
       "          [ 8.6700e+02,  2.6850e+03,  1.7800e+03, ..., -6.3030e+03,\n",
       "           -6.6100e+02,  5.7800e+02],\n",
       "          [-3.0000e+02, -3.0810e+03,  4.8970e+03, ...,  1.9080e+03,\n",
       "            4.8590e+03,  3.0600e+03]],\n",
       " \n",
       "         [[-2.7700e+02,  4.7590e+03,  3.9450e+03, ..., -7.0630e+03,\n",
       "            1.0130e+03, -6.2000e+02],\n",
       "          [ 2.1750e+03,  2.9190e+03,  3.1640e+03, ..., -1.7740e+03,\n",
       "           -7.4730e+03,  5.6980e+03],\n",
       "          [ 1.2420e+03, -2.2840e+03,  2.9080e+03, ..., -1.6150e+03,\n",
       "           -8.8620e+03,  3.6620e+03],\n",
       "          ...,\n",
       "          [-9.9200e+02,  3.7000e+02, -2.4940e+03, ...,  6.9300e+02,\n",
       "            5.2400e+02, -6.0460e+03],\n",
       "          [ 8.4700e+02,  2.7100e+03,  4.3800e+02, ..., -3.4770e+03,\n",
       "           -8.6840e+03,  5.6490e+03],\n",
       "          [-9.0000e+01, -8.0470e+03,  1.3380e+03, ...,  3.9610e+03,\n",
       "           -5.6000e+02,  1.1330e+04]]]], dtype=float32),\n",
       " array([ 1892352., -2441216., -8839168., -2217984., -5515264.,   253952.,\n",
       "         3362816., -3559424.,  2906112., -4638720., -3897344., -4843520.,\n",
       "          206848.,  5189632., -3493888., -3096576., -2387968., -5861376.,\n",
       "        -1202176., -2693120.,  -933888., -4182016.,   976896., -3522560.,\n",
       "        -2607104.,  -684032.,   786432.,  -901120.,  -931840.,  -700416.,\n",
       "         1177600., -6057984.], dtype=float32),\n",
       " array([[[[  -198.,    485.,    989., ...,  -3353.,    129.,  -7941.],\n",
       "          [ -5376.,  -4837.,  -4977., ...,  -5470.,    149.,   3447.],\n",
       "          [  1722.,  -4853.,  -2254., ...,  -4377.,    571.,   3638.],\n",
       "          ...,\n",
       "          [    23.,  -2917.,   1865., ...,  -8291.,   -213.,  -1295.],\n",
       "          [ -1784.,  -8055.,   -552., ...,  -4555.,   -386.,   -795.],\n",
       "          [  3281.,    727.,   2254., ...,  -1321.,   -117.,  -2690.]],\n",
       " \n",
       "         [[  -413.,   2591.,    714., ...,   -770.,     60.,  -4308.],\n",
       "          [ -2193.,  -1728.,  -2320., ...,    699.,   -865.,  -1668.],\n",
       "          [  1393.,    182.,   1006., ...,  -3228.,   -621.,  -1430.],\n",
       "          ...,\n",
       "          [  1219.,   1795.,   2413., ...,  -3104.,  -1163.,   -958.],\n",
       "          [ -1540.,  -3624.,   2316., ...,    534.,    399.,  -6480.],\n",
       "          [    53.,   -636.,    637., ...,    677.,  -1233.,    -28.]],\n",
       " \n",
       "         [[  3513.,   -499.,   1691., ...,   1317.,  -2067.,  -3589.],\n",
       "          [  2770.,  -1551.,    170., ...,   3242.,  -1512.,  -4156.],\n",
       "          [   861.,   -418.,   1910., ...,  -2493.,  -1914.,  -3165.],\n",
       "          ...,\n",
       "          [ -1010.,   1560.,   -848., ...,  -2771.,   -978.,  -4623.],\n",
       "          [ -1819.,   -626.,   4170., ...,   4673.,  -1625.,  -8897.],\n",
       "          [ -4627.,   -552.,  -3250., ...,   4779.,  -1394.,   1071.]]],\n",
       " \n",
       " \n",
       "        [[[   310.,   1411.,   1220., ...,    744.,   -114.,  -4297.],\n",
       "          [ -1170.,    839.,  -2180., ...,  -1360.,    768.,   4563.],\n",
       "          [  -199.,   1679.,  -1823., ...,   1806.,  -1199.,   1460.],\n",
       "          ...,\n",
       "          [   583.,  -4056.,  -2578., ...,  -4921.,   -203.,   2981.],\n",
       "          [   705.,   -705.,   3533., ...,  -4598.,   1147.,  -8258.],\n",
       "          [  1377.,  -1385.,   1629., ...,    712.,    273.,  -1782.]],\n",
       " \n",
       "         [[ -2751.,  -1460.,   -324., ...,   2708.,  -1281.,  -1674.],\n",
       "          [ -1283.,    720.,   1639., ...,   1022.,    803.,   -202.],\n",
       "          [   140.,   1698.,   1601., ...,   1906.,    635.,  -2271.],\n",
       "          ...,\n",
       "          [   -93.,   -688.,   -381., ...,  -4240.,  -1060.,    748.],\n",
       "          [ -1708.,   -845.,   2701., ...,   1424.,    644.,   1287.],\n",
       "          [ -1136.,   1917.,   -314., ...,   1892.,  -1359.,   1641.]],\n",
       " \n",
       "         [[   322.,  -5201.,    933., ...,    953.,  -1395.,   -358.],\n",
       "          [  1445.,    880.,  -1371., ...,    979.,  -1467.,    922.],\n",
       "          [  3086.,    604.,    989., ...,   3153.,    406.,  -3183.],\n",
       "          ...,\n",
       "          [   146.,   1873.,  -2484., ...,  -2244.,   -258.,    823.],\n",
       "          [ -5324.,  -5538.,    831., ...,   1982.,   -732.,   -194.],\n",
       "          [ -7770.,   2711.,  -1517., ...,   1381.,  -1848.,    772.]]],\n",
       " \n",
       " \n",
       "        [[[-11576.,   1925.,    832., ...,   1364.,  -1357.,  -8280.],\n",
       "          [ -2746.,   4922.,    262., ...,   -349.,   -121.,    972.],\n",
       "          [  -632.,   6061.,    109., ...,   2863.,   -536.,  -8626.],\n",
       "          ...,\n",
       "          [  4096.,  -3319.,  -3525., ...,  -1626.,     63.,   3926.],\n",
       "          [ -5878.,   1341.,   3971., ...,   2628.,   -142.,  -8776.],\n",
       "          [  1850.,  -5034.,   2188., ...,    295.,    272.,  -1439.]],\n",
       " \n",
       "         [[ -6329.,  -1347.,   1941., ...,   -739.,    398.,  -1012.],\n",
       "          [ -1858.,   1990.,    671., ...,    429.,   -699.,     50.],\n",
       "          [    22.,   4329.,   1311., ...,   2879.,    312.,  -3560.],\n",
       "          ...,\n",
       "          [  -353.,   -775.,     71., ...,   -443.,    971.,    821.],\n",
       "          [-13084.,  -1607.,     65., ...,    897.,   -450.,  -2153.],\n",
       "          [ -3930.,  -4582.,    612., ...,   -538.,  -2361.,   -749.]],\n",
       " \n",
       "         [[ -1255.,  -4859.,   2312., ...,  -3134.,  -1199.,   -290.],\n",
       "          [ -5494.,   3152.,    643., ...,   -484.,   -976.,  -2129.],\n",
       "          [ -1091.,    465.,     72., ...,   -329.,    740.,  -1861.],\n",
       "          ...,\n",
       "          [  3544.,  -2241.,  -1317., ...,   3276.,    377.,    487.],\n",
       "          [-14025.,  -4514.,   1225., ...,    270.,  -1411.,  -1958.],\n",
       "          [ -5545.,   -514.,    472., ...,  -3885.,  -2513.,   -843.]]]],\n",
       "       dtype=float32),\n",
       " array([ 3.461120e+05, -2.685440e+05, -2.841600e+05, -5.742080e+05,\n",
       "        -1.536000e+03, -9.469440e+05,  1.056256e+06, -1.667584e+06,\n",
       "        -1.244160e+05, -1.375744e+06,  2.680320e+05, -7.247360e+05,\n",
       "        -2.286080e+05,  7.083520e+05, -5.524480e+05, -1.658880e+05,\n",
       "        -1.725184e+06, -1.139456e+06, -3.328000e+03, -1.111040e+06,\n",
       "         2.547200e+05,  4.794880e+05, -8.107520e+05, -1.175040e+05,\n",
       "         7.961600e+04, -1.292800e+05, -5.647360e+05, -2.508800e+05,\n",
       "        -8.960000e+03, -3.722240e+05, -4.625920e+05,  4.938240e+05,\n",
       "        -4.682240e+05, -1.155584e+06, -1.994240e+05,  1.681920e+05,\n",
       "         8.788480e+05, -3.596800e+05, -5.591040e+05,  7.848960e+05,\n",
       "        -1.255424e+06, -2.521600e+05, -1.156352e+06, -1.400320e+06,\n",
       "        -6.504960e+05, -4.661760e+05,  3.261440e+05, -6.400000e+04,\n",
       "         6.809600e+04, -4.779520e+05, -1.297920e+05,  4.741120e+05,\n",
       "        -7.326720e+05, -8.371200e+04,  2.560000e+05, -1.305600e+05,\n",
       "         1.753600e+05, -7.180800e+05, -3.563520e+05, -4.070400e+05,\n",
       "        -6.973440e+05, -2.790400e+04, -3.847680e+05,  4.280320e+05],\n",
       "       dtype=float32),\n",
       " array([[ 1069., -7851.,  7556., ...,  6478., -6197.,  3055.],\n",
       "        [ -126.,   231.,  2581., ..., -9094., -1411.,  1894.],\n",
       "        [ -874.,  -204., -2751., ...,  2442., -2312.,   622.],\n",
       "        ...,\n",
       "        [  528., -2540.,  -775., ...,  6623., -6103., -1339.],\n",
       "        [ -315.,  -326., -2565., ...,  1809.,   494.,   395.],\n",
       "        [ 1717.,   373.,  2875., ..., -3641.,  6431.,  5653.]],\n",
       "       dtype=float32),\n",
       " array([-1.17504e+05,  1.62688e+05, -9.79200e+04,  1.60512e+05,\n",
       "         7.23840e+04,  2.98432e+05, -5.66400e+04,  9.15200e+04,\n",
       "         4.37760e+04,  5.76000e+04, -1.74080e+04, -4.92160e+04,\n",
       "         7.05280e+04, -4.27520e+04, -7.13600e+04,  1.53088e+05,\n",
       "         2.65472e+05,  2.04224e+05,  3.73120e+05, -1.02400e+03,\n",
       "         3.77408e+05,  3.39968e+05,  2.68160e+05, -7.81440e+04,\n",
       "        -2.45760e+04, -1.51808e+05,  1.69152e+05, -4.43520e+04,\n",
       "        -5.26720e+04,  1.52960e+05, -3.85920e+04,  1.93280e+04,\n",
       "        -8.65920e+04,  1.63904e+05,  1.24800e+05, -2.73280e+04,\n",
       "        -1.49888e+05,  1.42016e+05,  7.85280e+04,  1.46240e+05,\n",
       "         1.00544e+05,  6.40000e+01,  7.23200e+03,  1.45920e+04,\n",
       "        -6.19520e+04,  3.54752e+05, -3.97440e+04,  1.51552e+05,\n",
       "        -5.56800e+03,  0.00000e+00, -1.77920e+04, -1.71392e+05,\n",
       "         0.00000e+00, -1.20448e+05,  6.84800e+04, -3.27680e+04,\n",
       "         1.97120e+04, -2.45760e+04,  1.26080e+05, -1.53856e+05,\n",
       "         3.25120e+05, -1.45920e+04,  9.93920e+04,  1.06432e+05],\n",
       "       dtype=float32),\n",
       " array([[  2502.,  -2869.,  -2681.,   -281.,    260.,   3105.,  -1521.,\n",
       "           2221.,  -4284.,   1235.],\n",
       "        [  -448.,  -8578.,   5905.,   2821.,   1939.,   1676.,  -3684.,\n",
       "           2123.,   2019.,   5932.],\n",
       "        [  3741.,   5978.,   4929.,  -1201.,  -6047.,   2828.,   1656.,\n",
       "            475.,  -9435.,   2122.],\n",
       "        [  1899.,  -3416.,   6293.,   2951.,  -6839.,  -5569.,  -1325.,\n",
       "           4685.,   1636.,   3498.],\n",
       "        [ -2772.,   7094.,   7686.,  -7630.,  -5406., -11486.,   -184.,\n",
       "            801.,  -2663.,  -5602.],\n",
       "        [ -1009.,   3119.,   1606.,  -3962.,    179.,  -6502., -15143.,\n",
       "           5917.,  -2830.,    183.],\n",
       "        [ -1183.,   -952.,   2063.,   1242.,  -1788.,   3730.,   2250.,\n",
       "            739.,  -3027.,   2190.],\n",
       "        [ -4996., -14434.,  -8930.,    776.,   7712.,   8547.,   4127.,\n",
       "           2005.,   4021.,  -6134.],\n",
       "        [  2704.,   1203.,  -5504.,   2643.,  -5932.,   7748.,   -816.,\n",
       "          -7268.,   3773.,  -9914.],\n",
       "        [  1399.,   9492.,  -3737.,  -9197., -11851.,   9187.,   4583.,\n",
       "         -11590.,   3445.,  -4841.],\n",
       "        [ -1235.,   -134.,   -691.,    611.,  -2528.,  -2955.,    -45.,\n",
       "           1170.,    765.,   4112.],\n",
       "        [ -2985.,    -57.,  -1030.,   1569.,  -2741.,  -3319.,  -4335.,\n",
       "           1377.,   3358.,    -50.],\n",
       "        [  7012.,  -9299.,  -3784.,   -270., -12279.,   2974.,  -1088.,\n",
       "            600.,   2082.,   3818.],\n",
       "        [ -5349.,  -9308.,  -2223.,    501.,  -2270.,  -4921.,   1166.,\n",
       "          -2567.,  -4694.,  -1651.],\n",
       "        [ -8321.,  -4431.,  -5625.,   5973., -13035.,   3582.,   5913.,\n",
       "          -1694.,    935.,  -5090.],\n",
       "        [  3401.,  -5113.,  -1709.,  -7036.,   -153.,   3778.,   6473.,\n",
       "           4583.,   3812.,   1903.],\n",
       "        [ 10133.,  10079.,  -3797.,  -1124.,  -7235.,  -5033., -10404.,\n",
       "           3990.,   3918.,  -4252.],\n",
       "        [  7021.,    920.,   1455.,   2469.,   7436.,    360., -12850.,\n",
       "          -2192.,   2820.,  -1707.],\n",
       "        [-14975.,   2661.,  -6997.,   -989.,   3374.,  -9582.,   -986.,\n",
       "           4381.,  -7827.,  -2536.],\n",
       "        [   878.,  -1267.,   2715.,  -8076.,   3899.,  -2707.,   5259.,\n",
       "          -6820.,    -83.,   2592.],\n",
       "        [  2180.,  -5587.,  -1941., -11773.,   -462.,   -721.,  -3257.,\n",
       "          -8085.,   9879.,   -209.],\n",
       "        [  2805.,    850.,   -232.,  -1355.,   5155.,  -4130.,  -7263.,\n",
       "           4831.,   5385.,   4183.],\n",
       "        [ -6028.,  -3890.,  -4246.,  -3974.,   2553.,  -1022., -16397.,\n",
       "           3987.,  -3173.,   1749.],\n",
       "        [ -3321.,   3412.,  -3750.,   2457.,  -1771.,    317.,    526.,\n",
       "           3274.,    357.,  -3492.],\n",
       "        [  6419., -15900.,  -5477.,    777.,  -9310.,  -4727.,   4052.,\n",
       "          -2394.,  -5889.,   7537.],\n",
       "        [  1411.,  -1937.,  -3615.,   4234.,   1881.,   4629.,   5363.,\n",
       "         -13301.,   1372.,  -5652.],\n",
       "        [  4646.,  -4561.,   -736.,  -6460.,  -4084.,  -7027.,    628.,\n",
       "          -4721.,   6206.,  -3442.],\n",
       "        [ -4527.,  -5633.,   -532.,    254.,  -2942.,   -308.,  -1156.,\n",
       "            -96.,  -2039.,  -2992.],\n",
       "        [ -1898.,  -2794.,   1112.,   2788.,   1691.,   2846.,    943.,\n",
       "            887.,   1743.,   1165.],\n",
       "        [  1933.,   6376.,  -5610.,  -5287.,   5016.,     82.,   -554.,\n",
       "           5076.,  -2493.,  -4603.],\n",
       "        [   724.,   5326.,  -5815.,   1263.,   2007.,  -2140.,   -745.,\n",
       "          -5930.,    652.,   -152.],\n",
       "        [-24534.,    154., -13298.,   3307.,  -1286.,   8333.,   -860.,\n",
       "          -5603.,  -2162.,   9335.],\n",
       "        [ -3489.,  -4860.,  -4914.,  -2394.,   1216.,   3153.,  -2818.,\n",
       "          -3520.,   1860.,   3754.],\n",
       "        [  5247.,  -3999., -10703.,  -2688.,   6465.,   3408.,  -1594.,\n",
       "           2996.,  -4863.,   2821.],\n",
       "        [ -8018.,   3178.,   4096.,  -1468.,   6275.,  -3345.,   -703.,\n",
       "           3633.,  -7407.,   2799.],\n",
       "        [  2432.,   -883.,  -4026.,  -2728.,  -3886.,   1952.,   2442.,\n",
       "           4393.,    585.,  -3593.],\n",
       "        [  6793.,  -2761.,    471.,   1067.,  -3411.,   3490.,   5199.,\n",
       "         -10321.,  -1284.,   2440.],\n",
       "        [  6925.,   6032.,   -571.,  -7476.,   2761.,   3784.,  -5390.,\n",
       "          -1667.,  -2214.,    499.],\n",
       "        [ -4668.,   6292.,  -3888.,  -2809.,   1734.,   5547.,  -9439.,\n",
       "           3416.,  -5461.,   8124.],\n",
       "        [ -2277.,   -810.,   6939.,  -2421.,  -8093.,   3655.,   -577.,\n",
       "          -4424.,  -6566.,   1670.],\n",
       "        [ -3332.,  -4288.,   2832.,   5231.,  -4642.,   4184.,  -7326.,\n",
       "           4681.,  -1294.,   -870.],\n",
       "        [-12101.,   -556.,    509.,   7268.,   1419.,    406., -17435.,\n",
       "          -7020.,   -813.,   8605.],\n",
       "        [ -7279., -10253.,   2726.,   5767.,  -7960.,   1284.,   6083.,\n",
       "          -3883.,   5415.,   6040.],\n",
       "        [   -81.,   6388.,  -4599.,    310.,   3578.,     55.,  -4819.,\n",
       "           2677.,   2033.,   -651.],\n",
       "        [  2063.,    977.,   1823.,   2067.,   2339.,    408.,   3889.,\n",
       "           -734.,  -3296.,   1442.],\n",
       "        [ -1754.,  -8012.,  -2251.,  -6168.,  -2886.,  -7339.,  -3515.,\n",
       "           2968.,   7502.,   3343.],\n",
       "        [  -300.,     94.,   -653.,   -697.,   2308.,   2853.,   1517.,\n",
       "          -1152.,  -3066.,   -187.],\n",
       "        [   459.,   5398.,   7505.,   6116., -13110.,  -5558.,   -141.,\n",
       "           4278.,   4123., -15124.],\n",
       "        [ -3188.,   2390.,   1481.,  -3816.,   -746.,  -4305.,  -1512.,\n",
       "          -2288.,  -4323.,   3547.],\n",
       "        [  3705.,  -4243.,    233.,  -2234.,   2907.,  -3091.,  -2224.,\n",
       "           1495.,  -1793.,  -4592.],\n",
       "        [  8203.,   9249.,  -6609.,   8862.,  -5905.,   -157.,   1253.,\n",
       "          -3236.,  -3369.,   2602.],\n",
       "        [  3863.,  -4094.,   2098.,   -948.,  -8426.,   5773.,   7392.,\n",
       "         -11778.,   1946.,   1951.],\n",
       "        [  1419.,  -2958.,  -1496.,   -818.,   3715.,   4202.,   2210.,\n",
       "           2860.,  -2401.,   2054.],\n",
       "        [ -7843.,  13004.,   6667.,   8683.,  -1785.,  -4281.,  -2668.,\n",
       "          -1288.,   7163.,    395.],\n",
       "        [ -1841.,   -372.,   7227.,  -7415.,   6591.,  -3558.,    183.,\n",
       "          -3877.,   3682.,   4251.],\n",
       "        [   519.,   1387.,   5315.,   -454.,   7144.,   3579.,   3052.,\n",
       "           3556.,  -5879.,  -4293.],\n",
       "        [ -4304.,  10709.,   6326.,   5708.,   -221.,    142.,  -3519.,\n",
       "           5302.,   2073., -11622.],\n",
       "        [   146.,  -5598.,  -6293.,   6933.,   5819.,  -6091.,   1940.,\n",
       "           -809.,   -268.,   1133.],\n",
       "        [  -989.,  10822.,  -6757.,   4649.,    594., -10151.,   7617.,\n",
       "           7261.,  -4312., -10474.],\n",
       "        [   874.,  -1605.,  -3310.,   1584.,  -2691.,  -4285.,   2545.,\n",
       "           1023.,   2250.,   1573.],\n",
       "        [ -4383.,   4990.,  -9234.,    749.,   5418.,    121.,     47.,\n",
       "           1742.,   6460.,   2658.],\n",
       "        [  1869.,   -497.,    366.,  -4537.,   2826.,  -6025.,   7368.,\n",
       "          -7733.,    517.,   -322.],\n",
       "        [ -4118.,  -6821.,   4531.,   4116.,  -1366.,   2659.,  -2254.,\n",
       "           5361.,  -1600.,  -5728.],\n",
       "        [ -1032.,  -1308.,   4345.,   -848., -14349.,   3905.,   3206.,\n",
       "           1205.,   5737., -16976.]], dtype=float32),\n",
       " array([ -324608.,   247808.,  -811008., -3604480.,  1918976., -2810880.,\n",
       "        -2341888.,  2214912.,  2761728.,  1393664.], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_weight_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(q_weight_list[i].shape) == 4:\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 2, 0))\n",
    "    else:\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 1, 0))\n",
    "        \n",
    "\n",
    "reordered_weight_flatten_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(reordered_weight_list[i].shape) == 4:\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten('F'))\n",
    "    else:\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********conv1********\n",
      "Bias Shift:  15\n",
      "Output Shift:   18\n",
      "Bias:   [   34  8424  7169  7528 -4077  6493  3365 11945  4238 -4259  -107 -1954\n",
      " -4130    66  8828 -1501]\n",
      "Weight:   [  3805   9568 -16052  -4213  22940   7086 -23831   3554   4424  -5958\n",
      " -16465   1733  14597  18937  20895   7545  -6191  -1796 -19357  -7097\n",
      "   4178  -4920   1137  12309  22822  14487   7658 -25462  12701  15027\n",
      "  12246  20921   4332  10542  -9557 -29017  17702  -7625  -7271  10889\n",
      "  -6855  -6480   9010  -7431  -4163  -4173  25360  12017 -26768  -1374\n",
      "  14156 -20788  -1407  11817  -7702 -20016  -9221  13048   3727   1673\n",
      "   3047  14149   5913  22197  17745  19735  18280  17453   5465  11497\n",
      "  -1881 -28805   5664  15052 -24021   8663  17579 -26740   6381  13122\n",
      "  -6729 -10594   -227   9584  -8045  -5755  11803  -6576  -2092   4470\n",
      "   6790   9371  13784  -8811    697   7296 -12929 -14443   -934  -8613\n",
      "  -5114  -9823    445  -6362  -9560  15176   7532  18558  -5282  -4341\n",
      "  -4869  -5404   -881   9515  -3646   5315   4000  -8296  -2947  -5509\n",
      "  -8906  -2954  -8706  -8591  -9480   -493  15614   6308  -5697  16863\n",
      "  10200   3127  11336  20129  24428    181   1838   -884  15067 -13213\n",
      "  -2816  17138 -16607   1684]\n",
      "********conv2********\n",
      "Bias Shift:  11\n",
      "Output Shift:   17\n",
      "Bias:   [  924 -1192 -4316 -1083 -2693   124  1642 -1738  1419 -2265 -1903 -2365\n",
      "   101  2534 -1706 -1512 -1166 -2862  -587 -1315  -456 -2042   477 -1720\n",
      " -1273  -334   384  -440  -455  -342   575 -2958]\n",
      "Weight:   [-2604 -2114 -2785 ... -6046  5649 11330]\n",
      "********conv3********\n",
      "Bias Shift:  8\n",
      "Output Shift:   16\n",
      "Bias:   [ 1352 -1049 -1110 -2243    -6 -3699  4126 -6514  -486 -5374  1047 -2831\n",
      "  -893  2767 -2158  -648 -6739 -4451   -13 -4340   995  1873 -3167  -459\n",
      "   311  -505 -2206  -980   -35 -1454 -1807  1929 -1829 -4514  -779   657\n",
      "  3433 -1405 -2184  3066 -4904  -985 -4517 -5470 -2541 -1821  1274  -250\n",
      "   266 -1867  -507  1852 -2862  -327  1000  -510   685 -2805 -1392 -1590\n",
      " -2724  -109 -1503  1672]\n",
      "Weight:   [ -198 -5376  1722 ...   487 -1958  -843]\n",
      "********fc1********\n",
      "Bias Shift:  6\n",
      "Output Shift:   10\n",
      "Bias:   [-1836  2542 -1530  2508  1131  4663  -885  1430   684   900  -272  -769\n",
      "  1102  -668 -1115  2392  4148  3191  5830   -16  5897  5312  4190 -1221\n",
      "  -384 -2372  2643  -693  -823  2390  -603   302 -1353  2561  1950  -427\n",
      " -2342  2219  1227  2285  1571     1   113   228  -968  5543  -621  2368\n",
      "   -87     0  -278 -2678     0 -1882  1070  -512   308  -384  1970 -2404\n",
      "  5080  -228  1553  1663]\n",
      "Weight:   [ 1069  -126  -874 ... -1339   395  5653]\n",
      "********fc2********\n",
      "Bias Shift:  10\n",
      "Output Shift:   16\n",
      "Bias:   [ -317   242  -792 -3520  1874 -2745 -2287  2163  2697  1361]\n",
      "Weight:   [  2502   -448   3741   1899  -2772  -1009  -1183  -4996   2704   1399\n",
      "  -1235  -2985   7012  -5349  -8321   3401  10133   7021 -14975    878\n",
      "   2180   2805  -6028  -3321   6419   1411   4646  -4527  -1898   1933\n",
      "    724 -24534  -3489   5247  -8018   2432   6793   6925  -4668  -2277\n",
      "  -3332 -12101  -7279    -81   2063  -1754   -300    459  -3188   3705\n",
      "   8203   3863   1419  -7843  -1841    519  -4304    146   -989    874\n",
      "  -4383   1869  -4118  -1032  -2869  -8578   5978  -3416   7094   3119\n",
      "   -952 -14434   1203   9492   -134    -57  -9299  -9308  -4431  -5113\n",
      "  10079    920   2661  -1267  -5587    850  -3890   3412 -15900  -1937\n",
      "  -4561  -5633  -2794   6376   5326    154  -4860  -3999   3178   -883\n",
      "  -2761   6032   6292   -810  -4288   -556 -10253   6388    977  -8012\n",
      "     94   5398   2390  -4243   9249  -4094  -2958  13004   -372   1387\n",
      "  10709  -5598  10822  -1605   4990   -497  -6821  -1308  -2681   5905\n",
      "   4929   6293   7686   1606   2063  -8930  -5504  -3737   -691  -1030\n",
      "  -3784  -2223  -5625  -1709  -3797   1455  -6997   2715  -1941   -232\n",
      "  -4246  -3750  -5477  -3615   -736   -532   1112  -5610  -5815 -13298\n",
      "  -4914 -10703   4096  -4026    471   -571  -3888   6939   2832    509\n",
      "   2726  -4599   1823  -2251   -653   7505   1481    233  -6609   2098\n",
      "  -1496   6667   7227   5315   6326  -6293  -6757  -3310  -9234    366\n",
      "   4531   4345   -281   2821  -1201   2951  -7630  -3962   1242    776\n",
      "   2643  -9197    611   1569   -270    501   5973  -7036  -1124   2469\n",
      "   -989  -8076 -11773  -1355  -3974   2457    777   4234  -6460    254\n",
      "   2788  -5287   1263   3307  -2394  -2688  -1468  -2728   1067  -7476\n",
      "  -2809  -2421   5231   7268   5767    310   2067  -6168   -697   6116\n",
      "  -3816  -2234   8862   -948   -818   8683  -7415   -454   5708   6933\n",
      "   4649   1584    749  -4537   4116   -848    260   1939  -6047  -6839\n",
      "  -5406    179  -1788   7712  -5932 -11851  -2528  -2741 -12279  -2270\n",
      " -13035   -153  -7235   7436   3374   3899   -462   5155   2553  -1771\n",
      "  -9310   1881  -4084  -2942   1691   5016   2007  -1286   1216   6465\n",
      "   6275  -3886  -3411   2761   1734  -8093  -4642   1419  -7960   3578\n",
      "   2339  -2886   2308 -13110   -746   2907  -5905  -8426   3715  -1785\n",
      "   6591   7144   -221   5819    594  -2691   5418   2826  -1366 -14349\n",
      "   3105   1676   2828  -5569 -11486  -6502   3730   8547   7748   9187\n",
      "  -2955  -3319   2974  -4921   3582   3778  -5033    360  -9582  -2707\n",
      "   -721  -4130  -1022    317  -4727   4629  -7027   -308   2846     82\n",
      "  -2140   8333   3153   3408  -3345   1952   3490   3784   5547   3655\n",
      "   4184    406   1284     55    408  -7339   2853  -5558  -4305  -3091\n",
      "   -157   5773   4202  -4281  -3558   3579    142  -6091 -10151  -4285\n",
      "    121  -6025   2659   3905  -1521  -3684   1656  -1325   -184 -15143\n",
      "   2250   4127   -816   4583    -45  -4335  -1088   1166   5913   6473\n",
      " -10404 -12850   -986   5259  -3257  -7263 -16397    526   4052   5363\n",
      "    628  -1156    943   -554   -745   -860  -2818  -1594   -703   2442\n",
      "   5199  -5390  -9439   -577  -7326 -17435   6083  -4819   3889  -3515\n",
      "   1517   -141  -1512  -2224   1253   7392   2210  -2668    183   3052\n",
      "  -3519   1940   7617   2545     47   7368  -2254   3206   2221   2123\n",
      "    475   4685    801   5917    739   2005  -7268 -11590   1170   1377\n",
      "    600  -2567  -1694   4583   3990  -2192   4381  -6820  -8085   4831\n",
      "   3987   3274  -2394 -13301  -4721    -96    887   5076  -5930  -5603\n",
      "  -3520   2996   3633   4393 -10321  -1667   3416  -4424   4681  -7020\n",
      "  -3883   2677   -734   2968  -1152   4278  -2288   1495  -3236 -11778\n",
      "   2860  -1288  -3877   3556   5302   -809   7261   1023   1742  -7733\n",
      "   5361   1205  -4284   2019  -9435   1636  -2663  -2830  -3027   4021\n",
      "   3773   3445    765   3358   2082  -4694    935   3812   3918   2820\n",
      "  -7827    -83   9879   5385  -3173    357  -5889   1372   6206  -2039\n",
      "   1743  -2493    652  -2162   1860  -4863  -7407    585  -1284  -2214\n",
      "  -5461  -6566  -1294   -813   5415   2033  -3296   7502  -3066   4123\n",
      "  -4323  -1793  -3369   1946  -2401   7163   3682  -5879   2073   -268\n",
      "  -4312   2250   6460    517  -1600   5737   1235   5932   2122   3498\n",
      "  -5602    183   2190  -6134  -9914  -4841   4112    -50   3818  -1651\n",
      "  -5090   1903  -4252  -1707  -2536   2592   -209   4183   1749  -3492\n",
      "   7537  -5652  -3442  -2992   1165  -4603   -152   9335   3754   2821\n",
      "   2799  -3593   2440    499   8124   1670   -870   8605   6040   -651\n",
      "   1442   3343   -187 -15124   3547  -4592   2602   1951   2054    395\n",
      "   4251  -4293 -11622   1133 -10474   1573   2658   -322  -5728 -16976]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./weight.h\", \"w\")\n",
    "for i in range(len(q_weight_list)):\n",
    "    print(\"********\" + layer_names[i] + \"********\")\n",
    "    print(\"Bias Shift: \", bias_shift_list[i])\n",
    "    print(\"Output Shift:  \", output_shift_list[i])\n",
    "    print(\"Bias:  \", q_bias_list[i].astype(np.int16))\n",
    "    print(\"Weight:  \", reordered_weight_flatten_list[i].astype(np.int16))\n",
    "    \n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \"_WT {\" + str(reordered_weight_flatten_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS {\" + str(q_bias_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS_LSHIFT \" + str(bias_shift_list[i]) + \"\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_OUT_RSHIFT \" + str(output_shift_list[i]) + \"\\n\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = []\n",
    "intermediate_output.append(q_model.get_layer('input').output)\n",
    "\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(q_model.get_layer(layer_names[i]).output)\n",
    "\n",
    "q_intermediate_layer_model = Model(inputs = q_model.input, outputs = intermediate_output)\n",
    "\n",
    "sample_output = q_intermediate_layer_model.predict(np.round(x_test[0: 1]*(2**frac_bits_activation_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 16)\n",
      "(14, 14, 32)\n",
      "(7, 7, 64)\n",
      "(64,)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./sample_input_output.h\", \"w\")\n",
    "temp = np.squeeze(sample_output[0], axis=0)\n",
    "temp = temp.transpose(2, 0, 1)\n",
    "temp = temp.flatten(\"F\")\n",
    "f.write(\"#define \" + \"INPUT_DATA {\" + str(temp.astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "\n",
    "for i in range(layer_num):\n",
    "    temp = np.squeeze(sample_output[i+1], axis=0)\n",
    "    print(temp.shape)\n",
    "    if len(temp.shape) == 3:\n",
    "        temp = temp.transpose(2, 0, 1)\n",
    "        temp = temp.flatten(\"F\")\n",
    "    \n",
    "    temp = temp.astype(np.int32) >> output_shift_list[i]\n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \" {\" + str(temp.tolist())[1:-1] + \"}\\n\") \n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
