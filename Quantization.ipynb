{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input, ReLU, GlobalAveragePooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "quantize numpy array\n",
    "return:\n",
    "q_x: quantized array\n",
    "fp_x: reverse quantized array to float\n",
    "\"\"\"\n",
    "def quantize_array(x, bit_depth=8):\n",
    "    min_x = x.min() \n",
    "    max_x = x.max()\n",
    "\n",
    "    #find number of integer bits to represent this range\n",
    "    int_bits = int(np.ceil(np.log2(max(abs(min_x),abs(max_x)))))\n",
    "    \n",
    "    if int_bits < 0: int_bits = 0\n",
    "\n",
    "    frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "\n",
    "    #floating point weights are scaled and rounded to [-128,127], which are used in \n",
    "    #the fixed-point operations on the actual hardware (i.e., microcontroller)\n",
    "    q_x = np.round(x*(2**frac_bits))\n",
    "\n",
    "    #To quantify the impact of quantized weights, scale them back to\n",
    "    # original range to run inference using quantized weights\n",
    "    fp_x = q_x/(2**frac_bits)\n",
    "    \n",
    "    return q_x, fp_x, int_bits, frac_bits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "compute frac_bits based on max and min list\n",
    "used for activation quantization\n",
    "\"\"\"\n",
    "def min_max_quantize(max_list, min_list, bit_depth=8):\n",
    "    frac_list = []\n",
    "    for i in range(len(max_list)):\n",
    "        int_bits = int(np.ceil(np.log2(max(abs(min_list[i]),abs(max_list[i])))))\n",
    "        if int_bits < 0: int_bits = 0\n",
    "        frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "        frac_list.append(frac_bits)\n",
    "    return frac_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original floating-point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (14544, 4096, 1, 1)\n",
      "Output shape:  (14544,)\n",
      "Accuracy:  0.8620737073707371\n"
     ]
    }
   ],
   "source": [
    "def Net():\n",
    "    x_in = Input(shape=(4096, 1, 1))\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid')(x_in)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(16, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x_out = Activation('sigmoid')(x)\n",
    "    return Model(x_in, x_out)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Load the dataset\n",
    "x = np.load(\"./data/input.npy\")\n",
    "y = np.load(\"./data/target.npy\")\n",
    "x = np.expand_dims(np.expand_dims(np.squeeze(x), -1), -1)\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", y.shape)\n",
    "\n",
    "# Load pre-trained weight\n",
    "model.load_weights(\"./weights/tf.h5\")\n",
    "\n",
    "# Evaluation (float 32)\n",
    "length = y.shape[0]\n",
    "positives = 0\n",
    "for i in range(length):\n",
    "    pred = model(x[i:i+1])\n",
    "    pred = (pred > 0.5)\n",
    "    positives += np.sum(y[i]==pred)\n",
    "\n",
    "print(\"Accuracy: \", positives / length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine the model\n",
    "\n",
    "1. define layer names\n",
    "\n",
    "2. add bitwise shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names:  ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'conv6', 'conv7', 'conv8', 'fc1', 'fc2', 'fc3']\n"
     ]
    }
   ],
   "source": [
    "def QNet(bit=32, shift_list=[]):\n",
    "    \n",
    "    shift = False\n",
    "    if bit != 32:\n",
    "        shift = True\n",
    "        \n",
    "    x_in = Input(shape=(4096, 1, 1), name=\"input\")\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid', name=\"conv1\")(x_in)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[0]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid', name=\"conv2\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[1]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(16, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv3\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[2]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv4\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[3]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv5\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[4]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv6\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[5]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv7\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[6]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv8\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[7]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1), name=\"max_pool\")(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D(name=\"pool\")(x)\n",
    "    if shift:\n",
    "        x = tf.floor(x)\n",
    "    \n",
    "    x = Dense(64, name=\"fc1\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[8]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(512, name=\"fc2\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[9]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(1, name=\"fc3\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[10]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    if not shift:\n",
    "        x_out = Activation('sigmoid')(x)\n",
    "    else:\n",
    "        x_out = x\n",
    "    \n",
    "    return Model(x_in, x_out)\n",
    "\n",
    "model = QNet()\n",
    "model.load_weights(\"./weights/tf.h5\")\n",
    "\n",
    "# get layer names\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    if len(layer.get_weights()) != 0:\n",
    "        layer_names.append(layer.name)\n",
    "print(\"Layer names: \", layer_names)\n",
    "layer_num = len(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  15\n",
      "1    conv1    14\n",
      "2    conv2    12\n",
      "3    conv3    11\n",
      "4    conv4    11\n",
      "5    conv5    10\n",
      "6    conv6    10\n",
      "7    conv7    9\n",
      "8    conv8    7\n",
      "9    fc1    7\n",
      "10    fc2    8\n",
      "11    fc3    9\n"
     ]
    }
   ],
   "source": [
    "# get intermediate tensor to calculate dynamic range \n",
    "intermediate_output = [model.get_layer(\"input\").output]\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(model.get_layer(layer_names[i]).output)\n",
    "intermediate_layer_model = Model(inputs = model.input, outputs = intermediate_output)\n",
    "\n",
    "\n",
    "x = np.load(\"./data/input.npy\")\n",
    "x = np.expand_dims(np.expand_dims(np.squeeze(x), -1), -1)\n",
    "length = x.shape[0]\n",
    "\n",
    "frac_bits_output_list = []\n",
    "max_list = []\n",
    "min_list = []\n",
    "for i in range(length):\n",
    "    pred = intermediate_layer_model(x[i: i+1])\n",
    "    for i in range(len(pred)):\n",
    "        pred_np = pred[i].numpy()\n",
    "        max_np = pred_np.max()\n",
    "        min_np = pred_np.min()\n",
    "        if len(max_list) != len(pred):\n",
    "            max_list.append(max_np)\n",
    "            min_list.append(min_np)\n",
    "        else:\n",
    "            if max_np > max_list[i]:\n",
    "                max_list[i] = max_np\n",
    "            if min_np < min_list[i]:\n",
    "                min_list[i] = min_np\n",
    "                \n",
    "\n",
    "bit_depth = 16\n",
    "frac_bits_activation_list = min_max_quantize(max_list, min_list, bit_depth=bit_depth)\n",
    "\n",
    "print(\"input: \", frac_bits_activation_list[0])\n",
    "for i in range(layer_num):\n",
    "    print(i+1, \"  \", layer_names[i], \"  \", frac_bits_activation_list[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "conv1 - - weight - - Q0.15\n",
      "conv1 - - bias - - Q0.15\n",
      "**********\n",
      "conv2 - - weight - - Q0.15\n",
      "conv2 - - bias - - Q0.15\n",
      "**********\n",
      "conv3 - - weight - - Q1.14\n",
      "conv3 - - bias - - Q0.15\n",
      "**********\n",
      "conv4 - - weight - - Q0.15\n",
      "conv4 - - bias - - Q0.15\n",
      "**********\n",
      "conv5 - - weight - - Q0.15\n",
      "conv5 - - bias - - Q0.15\n",
      "**********\n",
      "conv6 - - weight - - Q0.15\n",
      "conv6 - - bias - - Q0.15\n",
      "**********\n",
      "conv7 - - weight - - Q0.15\n",
      "conv7 - - bias - - Q0.15\n",
      "**********\n",
      "conv8 - - weight - - Q0.15\n",
      "conv8 - - bias - - Q0.15\n",
      "**********\n",
      "fc1 - - weight - - Q0.15\n",
      "fc1 - - bias - - Q0.15\n",
      "**********\n",
      "fc2 - - weight - - Q0.15\n",
      "fc2 - - bias - - Q0.15\n",
      "**********\n",
      "fc3 - - weight - - Q0.15\n",
      "fc3 - - bias - - Q0.15\n"
     ]
    }
   ],
   "source": [
    "bit_depth = 16\n",
    "frac_bits_weight_list = []\n",
    "frac_bits_bias_list = []\n",
    "q_weight_list = []\n",
    "q_bias_list = []\n",
    "\n",
    "for i in range(layer_num):\n",
    "    weight, bias = model.get_layer(layer_names[i]).get_weights()\n",
    "    q_weight, f_weight, int_bits_weight, frac_bits_weight = quantize_array(weight, bit_depth=bit_depth)\n",
    "    q_bias, f_bias, int_bits_bias, frac_bits_bias = quantize_array(bias, bit_depth=bit_depth)\n",
    "    \n",
    "    q_weight_list.append(q_weight)\n",
    "    q_bias_list.append(q_bias)\n",
    "    frac_bits_weight_list.append(frac_bits_weight)\n",
    "    frac_bits_bias_list.append(frac_bits_bias)\n",
    "    print(\"**********\")\n",
    "    print(layer_names[i] + \" - - weight - - Q\"+str(int_bits_weight)+\".\"+str(frac_bits_weight))\n",
    "    print(layer_names[i] + \" - - bias - - Q\"+str(int_bits_bias)+\".\"+str(frac_bits_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 14 11 11 11 10 10  9  7  7  8]\n",
      "[16 17 15 15 16 15 16 17 15 14 14]\n"
     ]
    }
   ],
   "source": [
    "bias_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_bias_list)\n",
    "output_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_activation_list[1:])\n",
    "print(bias_shift_list)\n",
    "print(output_shift_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_weight_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(q_weight_list[i].shape) == 4:\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 2, 0))\n",
    "    else:\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_weight_flatten_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(reordered_weight_list[i].shape) == 4:\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten('F'))\n",
    "    else:\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********conv1********\n",
      "Bias Shift:  15\n",
      "Output Shift:   16\n",
      "Bias:   [-6987 -1591 -3953 -1558  -551 -1170 -1919 -8994  1757  3595  1367   386\n",
      "  4719   948 -1146 -4442]\n",
      "Weight:   [ -5133  -6346  15618   8046   1626 -11557  -6722   4730  14606 -15771\n",
      "   2909  13927  -7468  -5517  13196  15111  -5435 -12725  12343  -2583\n",
      "  20424  -6902  15284   6531   2958   8492   5624   8721  11169  -4637\n",
      "   8505  -5351  -6712    -82 -17356  -1567 -10004   4937  14367  13303\n",
      "  -7640 -14677 -11588   3580   7887  -4306 -14985 -15087   3479  -7283\n",
      "  -9485  15265 -16571   8921   3602 -12091  -5791  -8984    357  -6791\n",
      "  -2968  -9818    286 -11545  10142  -2565  -7413   -885  -9787   5731\n",
      "  -9628  -6136  10717  -4297   3869 -12797 -11801  17536  -3577  20251\n",
      " -19126   5947   1169  -6379  -4265  -9967  -8463  15307  -4395  15455\n",
      " -14937  12278 -14131  10988  -4144  -4663  19997  -7483   8577 -18633\n",
      "   3830 -12976   2984    895  -5607   8771   2134   4207   3172   2142\n",
      "  11329  -5499  16072   -651  -2559   5537 -18222  -1357  -5780   4803\n",
      "  14906 -14565  -7593  17165 -18093   9029  12587  -6861  14307   9524\n",
      "  14588  -5019 -16800   9281  -4046 -13366 -14970 -16819  14735  16822\n",
      "   -567  14798   -971   2689]\n",
      "********conv2********\n",
      "Bias Shift:  14\n",
      "Output Shift:   17\n",
      "Bias:   [-2563 -4566 -4500  -653 -5059 -3098   286   262  -493  1159 -5108 -3230\n",
      " -3492 -1381  -101 -3447]\n",
      "Weight:   [-664 5528 3404 ... -354  877 1645]\n",
      "********conv3********\n",
      "Bias Shift:  11\n",
      "Output Shift:   15\n",
      "Bias:   [ 1470 -2978  4172  1166  6971 -7288  1291 -4779   303  1275 -1145  -650\n",
      " -5676  1853  1808  4895   748 -1777  2595  -233  1986  2801  6141 -1697\n",
      " -6868 -5924  -342 -5528   339  -245 -9117 -5798]\n",
      "Weight:   [ 4209  5614  3658 ...  2738 -4852  2300]\n",
      "********conv4********\n",
      "Bias Shift:  11\n",
      "Output Shift:   15\n",
      "Bias:   [-1490  2241 -5725 -3521  -681  1386  -921  1265   103 -3976 -6064 -6191\n",
      "  1450  1694 -1877 -5068    -5   768   782  4105 -2525 -2577 -4392   241\n",
      " -3973  -273 -3208   728  2982  2094  -342  1513]\n",
      "Weight:   [-5893   715  4054 ... -7515   759  4234]\n",
      "********conv5********\n",
      "Bias Shift:  11\n",
      "Output Shift:   16\n",
      "Bias:   [-5506  -995 -3639   210   235 -4258   633 -1155  3179   984   395  -494\n",
      "  1534 -5963 -1472    58 -5764  1801  -480 -1050   849 -2041  1660  -611\n",
      " -1366   788 -2257  -865 -1464   473 -5585 -3463]\n",
      "Weight:   [ 9477  3291  2344 ... -2656 -2704 -4045]\n",
      "********conv6********\n",
      "Bias Shift:  10\n",
      "Output Shift:   15\n",
      "Bias:   [  2326 -13148  -5219   1358  -5878   1639  -5254   2050   2948  -1389\n",
      "   1013  -1943   1756  -1779   2159  -4528  -4770   -654   1489  -4819\n",
      " -11497  -5278   3023  -1815  -3321   2598   -111  -3060   2469  -7826\n",
      "    846    729]\n",
      "Weight:   [ 4135 -3942 -8590 ...   285  8172 -4870]\n",
      "********conv7********\n",
      "Bias Shift:  10\n",
      "Output Shift:   16\n",
      "Bias:   [ -6340   -143   5773  -4372  -1757    109   -923  -2175   -272   -171\n",
      "  -4446    821    403   2867  -1244   2868   1498   1649   -283   -375\n",
      "  -2779   -557  -1385  -7795     91    665  -1894  -6203  -3401  -3345\n",
      "  -8227 -13126   1428   6413    101   1897   2771   6918   2468  -1323\n",
      "    448   -568  -1547    508   -770     55    -29   -115   -338   -430\n",
      "   3976   4958   1689    814  -6280   3441   1175  -2253   3436  -4008\n",
      "   1838    -69    647   5126]\n",
      "Weight:   [-2207  5389 -7313 ... -2188 -2074 -2537]\n",
      "********conv8********\n",
      "Bias Shift:  9\n",
      "Output Shift:   17\n",
      "Bias:   [ -1515  -2484   6181  -1800   3147   1994  -1877    897 -10719    -91\n",
      "  -3740   4588   -315   6806    897   9639   2840  -4739  -6763   2170\n",
      "  -3555   -238   5016  -3285  -3319  -3276  -2469   2718   4023  -5336\n",
      "   -242    857  -3855   7177  -3021  -1609  -5302  -2790   7015  -1798\n",
      "  -3351   -727  -3746   2911    210  -2707  -2380  -1844  -5903 -11208\n",
      "  -4722   -200   1690  -3552   4729  -3087   6244  -4802   3524  -8361\n",
      "   2795    491   2830  -6722]\n",
      "Weight:   [-1683 -7934 -2542 ... -1749  3089  1113]\n",
      "********fc1********\n",
      "Bias Shift:  7\n",
      "Output Shift:   15\n",
      "Bias:   [   712   -105    183   3316 -16577   6743  12606    280   2026   5637\n",
      "   -673   6193   5642   2198  10435  -1156   3812   1078  -8647   1589\n",
      "   2331  -1349  -2440  -3247  -4008  -4858   1566  -9486  -4297 -14592\n",
      "   6779  -3735  -3168  -3391  -3927  -5727   9200   4110  -2505  -6973\n",
      "   2029   3209  -5465   -703  -3527 -16876   4977    986  -1649   2183\n",
      "  -1874    270  12698     74   1676  -5920  -5228  18535   8014 -14794\n",
      "  11344   4035  -7519   -631]\n",
      "Weight:   [ 3588  6225  2500 ... -1228  1239 -3238]\n",
      "********fc2********\n",
      "Bias Shift:  7\n",
      "Output Shift:   14\n",
      "Bias:   [-16523    457  -4846  16523   -940   5098 -12020  -2408   3890  23362\n",
      "  10593  -4545   1631  11427  11343  20401   6492   4786  -5943  -2561\n",
      " -12549  11468 -11102  19895   2710 -11886   6617  -1044   1911   2266\n",
      "  -8044  26670    634  -1946   4301  18947  -8874  -3788  13198   5496\n",
      "   5267  16027  26127   8199   6641 -10731   6710   3505  15717  11653\n",
      "  16102  -1514  -5751   4809   3620   4320   7533  -3722   9429   2928\n",
      " -15402 -16937    281 -13336    511   2849  14344    849   5752 -10533\n",
      "   4376 -19791   2023  -7187  -8229   9679   4801   4856   -547  -2143\n",
      " -13577   2067   6169   4190  18495 -16307   4485   9158   9945  -8745\n",
      "   1508  24801   1985  12699  18438    417   -281  21287 -14448  -4672\n",
      "  -4660  11408  -2855 -11445  -3594 -13417   4226  -1713  -1079  13266\n",
      "  24107   8563  14891   4352 -26521   9047 -21648   4094   9003   8900\n",
      "    170  -2604  13835  -4463     33  -4768  15845  13367   1794   6051\n",
      "  -2932   3717   9914  -3962  -2305  17530   5943  19008  24830   -292\n",
      "  10353   7109  -5354  17785  15767  15318  -9326   2212  -8286  12641\n",
      "  -2324    317   4966  -1720    127   -756  -9622 -15216 -12506  -4300\n",
      "  -1676   5182   1378   9543  -6591   3339  -3118   -492   -295  -4985\n",
      "   1882   4179   4870 -12897 -26101  18943   4508   8568   6142  -1235\n",
      "   3558   8475   6217  -8259   6854  -6137  12516   4335 -26270   5425\n",
      "  -1372  27878   5009 -13413  -8084   1248   5456  -8703   8275  16963\n",
      "  -3770 -11233 -12121   5831   5578   4459 -24323   5148  -4220  -9113\n",
      "  20620  18885   2637    643   -770 -16338   2524    182 -14292   1562\n",
      "  19499  16131   5326  -3755  -2350   6486  15779   1362  -4179  -3744\n",
      "  -6382   8868 -23117  -7055  11590  -4100   3277  -5096   4082  -1178\n",
      "  21693   9343 -16422   -653   -335 -11842  -4055  11272  15712   1693\n",
      "  19711  -1026  -2254    186  13903   4250  12561  19311   3899  10326\n",
      "   2066  -6032  10515  -8448  12036   7593  -3800    224  15164  -3913\n",
      " -12781   5884   2340 -20147  29431  10006  15562   3163   5881  -3052\n",
      "  -3537  29320  -6365  -5398   5395   8844  14162   9534  -5882  -4379\n",
      "   1197  22470    993   6956  -2801   7105 -14616   8980    392   1925\n",
      " -13390 -23717  -1577     80  13306  27627  -5053   1435   3774   4652\n",
      "   8758  -4708   5961   4889  -3884   1658   5078  18512 -18737   9081\n",
      "  -3578  -7177   7243  -6133  -2247  11775  -7577  -1952   3752  10087\n",
      " -13665  13507  14402 -13090 -14406  10527   2045   -675   4820  -8563\n",
      "   8399  19229   5084   -501  25711   1761  15961   -219  -4166  14150\n",
      "     40  -4966   2267  -4544  17213  -2381   4458  -8388   1534 -14923\n",
      "  -1984  24323  -1166   5584  -5638  19505  14524  11292  19322  15105\n",
      "  -3029  22218   3848   -205    496  -4197   3509   9183 -11967   5987\n",
      "   -595  21190  -9495  21528  29034  -2149   6417  -1051  -2590    827\n",
      "  -5613  -5094  11998   7454  -4457  -6621 -10782   8586  13080 -10172\n",
      "  21760  -5315  -5089 -14271  11141  -1884  -3423  -4234  23805  -3362\n",
      "  -3579  -1746  -2522   6193  -8330  -1258 -13052  -1873  -2052    829\n",
      "  -4923  11954  -7112   -896   1123   6191  11490  -2357   9780  -9408\n",
      "  17496  -7296 -22016   3944    -36   5019   9193   6005  -9123  -2078\n",
      "  -3639  -8771   1390  16659  16910  -2720  -2821  18082  -3822  -3426\n",
      "  -4658 -15652   7788   2681  -1415 -10266  15352  -6975   9577  -8618\n",
      "  10926  12347  -3254  -4277  -6055  10606 -14036  -2813  -3894   6293\n",
      "  17920 -12078  -1055  -2112   7960  -8703 -21620  -2038 -13298   4000\n",
      "   6627  -1555  -4039  27202 -12986  -2377   4723  24063  -2677  -4490\n",
      "  22974   8091  16842    644  20267  17349  -3796   4107   2282   -758\n",
      "  14917  -1630   6086 -25190  -1383 -10025  -1402   3729   8984  11359\n",
      "  -3668   3596]\n",
      "Weight:   [-5071 -9440 -2309 ... -1228 -2377 -3802]\n",
      "********fc3********\n",
      "Bias Shift:  8\n",
      "Output Shift:   14\n",
      "Bias:   [3260]\n",
      "Weight:   [  3351  -1756   -143  -1273   -553  -2744   -680   -679  -1369  -2019\n",
      "   -946   -470   2903   -903   1450   1621  -3504    315  -1017   1264\n",
      "  -4085  -6218  -1368  -1362  -1245  -1918  -2203    -65  -2264    761\n",
      "  -1248   2743   1260   -326   1862    334   1441   -296  -3986   2026\n",
      "  -2643  -7032   1272  -2458  -2851   2798   -252    -37    580  -4934\n",
      "   -958    570     -6    845    591  -9574  -5729  -2714   1036   2171\n",
      "  -1199   1705    762   2571  -3435   1185   1536  -9063  -4625  -2874\n",
      "    387   1513   1393   -995   -348   5407  -1039   2131   2598  -2199\n",
      "  -1420   4465   3333   -502  -2467   5743  -1246   1854   1528  -1637\n",
      "  -5084   3296  -5771   1596    509   -518  -1139  -2178  -2059  -1975\n",
      "   1165   1413  -1193  -3885   -470    262  -8576    -75  -1055   -915\n",
      "  -2285   -466  -1061  -1057    410   2979   1188     95  -2199  -3185\n",
      "   -814    -37    -23  -2339   -202   -116  -9489  -1323   -650   1292\n",
      "   1703    -27  -4248    -75    363    443  -2515  -6995  -1089  -3149\n",
      "  -1146   2327  -1144  -3377  -2781   1844  -2182  -1309  -4498   -823\n",
      "   1206  -7982  -1080  -1377   -279   1119  -1661  -2890  -1700  -1086\n",
      " -10187   2130   1323      5  -1592  -2495   3446    605    762    312\n",
      "    -18   3199   4692   5180    418  -1871  -2555   2042   -793    318\n",
      "   1293   -990   1250    449    131     61  -5111  -1315     32  -2146\n",
      "    409   2263   2827   -427   -152   7354   2407   1540   2309    921\n",
      "   1187  -2068  -1716  -6257    474    561    406  -2184   -887  -3121\n",
      "  -1751 -15086   -712   1491   -611   -840  -1005   -206   -347    543\n",
      "   1422  -2139   3677    667    915     72   -768  -2781    751  -2212\n",
      "   -332   1540   -337  -6793  -1172  -1208   -988  -2721   -278   -721\n",
      "  -2456  -1255    306    229  -1733    344  -1406  -7873   3795    813\n",
      "    315    305  -1092    509  -8857  -3079   1294   1858  -3186   -742\n",
      "    460   1356   2549    415  -3903    750  -7144   4600   2238   4531\n",
      "    776    221  -2377  -4824   1396  -2540  -7290   2235  -1494   2024\n",
      "    289   1324  -1174   -775   1803  -1440     26   2479    248   1527\n",
      "  -2161   3328   -822   -497  -5059  -1468    149  -3905   2070   5641\n",
      "    161    562    661  -1441   1323   1743   -144    243  -3573  -3051\n",
      "   -154   -405   1373   1310   -749  -2389   -196   1002  -3754   7326\n",
      "   -489    202   1514    367     86   1065    276   1733   1791  -3944\n",
      "   -571  -2261    859    -78   -648  -2620  -1746    448   1156    294\n",
      "  -3261    921   2963  -1664   1883   2064  -8696  -3091  -1735   1274\n",
      "  -1143  -3971   -542   -673   2417     -3  -1633   -578  -2116  -1868\n",
      "    333   3819   2338    868  -6545   2578    695   3406   2335   -334\n",
      "  -1441  -6365    969   1180    302  -2625  -1008  -2099   -800   -160\n",
      "   3402  -3982  -1212   2116    939    432   4020    901     96   2987\n",
      "   1241   1342  -4839  -7421   1408   1288    432   1220   2024   -579\n",
      "  -2395    154    928    486    519    -75  -1430   -422   1548   -840\n",
      "    436   1186    747    123    220   1172  -1931  -1460  -5366   1953\n",
      "   1141   2452    911   1396   1922  -1974   3145   -108    809    -66\n",
      "   1467    -23  -1051   -420   -650  -1626    646  -5219  -4138  -1800\n",
      "   -777  -5044   -904 -10758   1194   1177    144  -4553   -630  -1631\n",
      "  -1013   -949   -373   -418   -946   1607 -20183   5006    330   1424\n",
      "   3301    273  -1220   1035   -605   -187   2113    717   2565  -3167\n",
      "   1602  -1678    764  -3147    231   1051   -335  -1074  -2035  -6340\n",
      "   1812   1075    962   1297   1012     69   7465    249   -187   -546\n",
      "   1541     67  -4578   2567    822    473   -116  -2213   1038   -724\n",
      " -12228   -841   3170    658    -98   -366   -774   5141  -3482   3666\n",
      "  -2065  -3075]\n"
     ]
    }
   ],
   "source": [
    "# f = open(\"./q_params.txt\", 'w')\n",
    "f = open(\"./weight.h\", \"w\")\n",
    "for i in range(len(q_weight_list)):\n",
    "    print(\"********\" + layer_names[i] + \"********\")\n",
    "    print(\"Bias Shift: \", bias_shift_list[i])\n",
    "    print(\"Output Shift:  \", output_shift_list[i])\n",
    "    print(\"Bias:  \", q_bias_list[i].astype(np.int16))\n",
    "    print(\"Weight:  \", reordered_weight_flatten_list[i].astype(np.int16))\n",
    "    \n",
    "#     f.write(\"********\" + layer_names[i] + \"********\" + \"\\n\")\n",
    "#     f.write(\"Bias Shift: \" + str(bias_shift_list[i]) + \"\\n\")\n",
    "#     f.write(\"Output Shift: \" + str(output_shift_list[i]) + \"\\n\")\n",
    "#     f.write(\"Bias:  \" + str(q_bias_list[i].astype(np.int16).tolist()) + \"\\n\")\n",
    "#     f.write(\"Weight:  \" + str(reordered_weight_flatten_list[i].astype(np.int16).tolist()) + \"\\n\")\n",
    "    \n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \"_WT {\" + str(reordered_weight_flatten_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS {\" + str(q_bias_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS_LSHIFT \" + str(bias_shift_list[i]) + \"\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_OUT_RSHIFT \" + str(output_shift_list[i]) + \"\\n\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = QNet(bit=16, shift_list=output_shift_list)\n",
    "\n",
    "for i in range(layer_num):\n",
    "    q_model.get_layer(layer_names[i]).set_weights([q_weight_list[i], q_bias_list[i]*(2**bias_shift_list[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8616611661166117\n"
     ]
    }
   ],
   "source": [
    "length = y.shape[0]\n",
    "positives = 0\n",
    "for i in range(length):\n",
    "    pred = q_model(np.round(x[i: i+1]*(2**frac_bits_activation_list[0])))\n",
    "    pred = (pred > 0)\n",
    "    positives += np.sum(y[i]==pred)\n",
    "\n",
    "print(positives / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = []\n",
    "intermediate_output.append(q_model.get_layer('input').output)\n",
    "\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(q_model.get_layer(layer_names[i]).output)\n",
    "\n",
    "q_intermediate_layer_model = Model(inputs = q_model.input, outputs = intermediate_output)\n",
    "\n",
    "sample_output = q_intermediate_layer_model.predict(np.round(x[0: 1]*(2**frac_bits_activation_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4088, 1, 16)\n",
      "(4080, 1, 16)\n",
      "(253, 1, 32)\n",
      "(251, 1, 32)\n",
      "(60, 1, 32)\n",
      "(58, 1, 32)\n",
      "(12, 1, 64)\n",
      "(10, 1, 64)\n",
      "(64,)\n",
      "(512,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./sample_input_output.h\", \"w\")\n",
    "temp = np.squeeze(sample_output[0], axis=0)\n",
    "temp = temp.transpose(2, 0, 1)\n",
    "temp = temp.flatten(\"F\")\n",
    "f.write(\"#define \" + \"INPUT_DATA {\" + str(temp.astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "\n",
    "for i in range(layer_num):\n",
    "    temp = np.squeeze(sample_output[i+1], axis=0)\n",
    "    print(temp.shape)\n",
    "    if len(temp.shape) == 3:\n",
    "        temp = temp.transpose(2, 0, 1)\n",
    "        temp = temp.flatten(\"F\")\n",
    "    \n",
    "    temp = temp.astype(np.int32) >> output_shift_list[i]\n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \" {\" + str(temp.tolist())[1:-1] + \"}\\n\") \n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
