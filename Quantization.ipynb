{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input, ReLU, GlobalAveragePooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMSIS NN quantization\n",
    "https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/deploying-convolutional-neural-network-on-cortex-m-with-cmsis-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "quantize numpy array\n",
    "return:\n",
    "q_x: quantized array\n",
    "fp_x: reverse quantized array to float\n",
    "\"\"\"\n",
    "def quantize_array(x, bit_depth=8):\n",
    "    min_x = x.min() \n",
    "    max_x = x.max()\n",
    "\n",
    "    #find number of integer bits to represent this range\n",
    "    int_bits = int(np.ceil(np.log2(max(abs(min_x),abs(max_x)))))\n",
    "    \n",
    "    if int_bits < 0: int_bits = 0\n",
    "\n",
    "    frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "\n",
    "    #floating point weights are scaled and rounded to [-2^(bit-1),2^(bit-1)-1], which are used in \n",
    "    #the fixed-point operations on the actual hardware (i.e., microcontroller)\n",
    "    q_x = np.round(x*(2**frac_bits))\n",
    "\n",
    "    #To quantify the impact of quantized weights, scale them back to\n",
    "    # original range to run inference using quantized weights\n",
    "    fp_x = q_x/(2**frac_bits)\n",
    "    \n",
    "    return q_x, fp_x, int_bits, frac_bits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "compute frac_bits based on max and min list\n",
    "used for activation(feature map) quantization\n",
    "\"\"\"\n",
    "def min_max_quantize(max_list, min_list, bit_depth=8):\n",
    "    frac_list = []\n",
    "    for i in range(len(max_list)):\n",
    "        int_bits = int(np.ceil(np.log2(max(abs(min_list[i]),abs(max_list[i])))))\n",
    "        if int_bits < 0: int_bits = 0\n",
    "        frac_bits = bit_depth - 1 - int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "        frac_list.append(frac_bits)\n",
    "    return frac_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original floating-point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (14544, 4096, 1, 1)\n",
      "Output shape:  (14544,)\n",
      "Accuracy:  0.8620737073707371\n"
     ]
    }
   ],
   "source": [
    "def Net():\n",
    "    x_in = Input(shape=(4096, 1, 1))\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid')(x_in)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(16, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x_out = Activation('sigmoid')(x)\n",
    "    return Model(x_in, x_out)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Load the dataset\n",
    "x = np.load(\"./data/input.npy\")\n",
    "y = np.load(\"./data/target.npy\")\n",
    "x = np.expand_dims(np.expand_dims(np.squeeze(x), -1), -1)\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", y.shape)\n",
    "\n",
    "# Load pre-trained weight\n",
    "model.load_weights(\"./weights/tf.h5\")\n",
    "\n",
    "# Evaluation (float 32)\n",
    "length = y.shape[0]\n",
    "positives = 0\n",
    "for i in range(length):\n",
    "    pred = model(x[i:i+1])\n",
    "    pred = (pred > 0.5)\n",
    "    positives += np.sum(y[i]==pred)\n",
    "\n",
    "print(\"Accuracy: \", positives / length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine the model\n",
    "\n",
    "1. define layer names\n",
    "\n",
    "2. add bitwise shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names:  ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'conv6', 'conv7', 'conv8', 'fc1', 'fc2', 'fc3']\n"
     ]
    }
   ],
   "source": [
    "def QNet(bit=32, shift_list=[]):\n",
    "    \n",
    "    shift = False\n",
    "    if bit != 32:\n",
    "        shift = True\n",
    "        \n",
    "    x_in = Input(shape=(4096, 1, 1), name=\"input\")\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid', name=\"conv1\")(x_in)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[0]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=(9, 1), strides=1, padding='valid', name=\"conv2\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[1]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(16, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv3\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[2]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv4\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[3]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv5\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[4]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv6\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[5]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1))(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv7\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[6]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 1), strides=1, padding='valid', name=\"conv8\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[7]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D(pool_size=(4, 1), name=\"max_pool\")(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D(name=\"pool\")(x)\n",
    "    if shift:\n",
    "        x = tf.floor(x)\n",
    "    \n",
    "    x = Dense(64, name=\"fc1\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[8]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(512, name=\"fc2\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[9]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(1, name=\"fc3\")(x)\n",
    "    if shift:\n",
    "        x = x / 2**shift_list[10]\n",
    "        x = tf.floor(x)\n",
    "        x = tf.clip_by_value(x, -2**(bit-1), 2**(bit-1)-1)\n",
    "    if not shift:\n",
    "        x_out = Activation('sigmoid')(x)\n",
    "    else:\n",
    "        x_out = x\n",
    "    \n",
    "    return Model(x_in, x_out)\n",
    "\n",
    "model = QNet()\n",
    "model.load_weights(\"./weights/tf.h5\")\n",
    "\n",
    "# get layer names\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    if len(layer.get_weights()) != 0:\n",
    "        layer_names.append(layer.name)\n",
    "print(\"Layer names: \", layer_names)\n",
    "layer_num = len(layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get intermediate feature maps to compute:\n",
    "\n",
    "1. the max and min value of each layer's output\n",
    "2. the quantization format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  15\n",
      "1    conv1    14\n",
      "2    conv2    12\n",
      "3    conv3    11\n",
      "4    conv4    11\n",
      "5    conv5    10\n",
      "6    conv6    10\n",
      "7    conv7    9\n",
      "8    conv8    7\n",
      "9    fc1    7\n",
      "10    fc2    8\n",
      "11    fc3    9\n"
     ]
    }
   ],
   "source": [
    "intermediate_output = [model.get_layer(\"input\").output]\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(model.get_layer(layer_names[i]).output)\n",
    "intermediate_layer_model = Model(inputs = model.input, outputs = intermediate_output)\n",
    "\n",
    "\n",
    "x = np.load(\"./data/input.npy\")\n",
    "x = np.expand_dims(np.expand_dims(np.squeeze(x), -1), -1)\n",
    "length = x.shape[0]\n",
    "\n",
    "frac_bits_output_list = []\n",
    "max_list = []\n",
    "min_list = []\n",
    "for i in range(length):\n",
    "    pred = intermediate_layer_model(x[i: i+1])\n",
    "    for i in range(len(pred)):\n",
    "        pred_np = pred[i].numpy()\n",
    "        max_np = pred_np.max()\n",
    "        min_np = pred_np.min()\n",
    "        if len(max_list) != len(pred):\n",
    "            max_list.append(max_np)\n",
    "            min_list.append(min_np)\n",
    "        else:\n",
    "            if max_np > max_list[i]:\n",
    "                max_list[i] = max_np\n",
    "            if min_np < min_list[i]:\n",
    "                min_list[i] = min_np\n",
    "                \n",
    "\n",
    "bit_depth = 16\n",
    "frac_bits_activation_list = min_max_quantize(max_list, min_list, bit_depth=bit_depth)\n",
    "\n",
    "print(\"input: \", frac_bits_activation_list[0])\n",
    "for i in range(layer_num):\n",
    "    print(i+1, \"  \", layer_names[i], \"  \", frac_bits_activation_list[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "conv1 - - weight - - Q0.15\n",
      "conv1 - - bias - - Q0.15\n",
      "**********\n",
      "conv2 - - weight - - Q0.15\n",
      "conv2 - - bias - - Q0.15\n",
      "**********\n",
      "conv3 - - weight - - Q1.14\n",
      "conv3 - - bias - - Q0.15\n",
      "**********\n",
      "conv4 - - weight - - Q0.15\n",
      "conv4 - - bias - - Q0.15\n",
      "**********\n",
      "conv5 - - weight - - Q0.15\n",
      "conv5 - - bias - - Q0.15\n",
      "**********\n",
      "conv6 - - weight - - Q0.15\n",
      "conv6 - - bias - - Q0.15\n",
      "**********\n",
      "conv7 - - weight - - Q0.15\n",
      "conv7 - - bias - - Q0.15\n",
      "**********\n",
      "conv8 - - weight - - Q0.15\n",
      "conv8 - - bias - - Q0.15\n",
      "**********\n",
      "fc1 - - weight - - Q0.15\n",
      "fc1 - - bias - - Q0.15\n",
      "**********\n",
      "fc2 - - weight - - Q0.15\n",
      "fc2 - - bias - - Q0.15\n",
      "**********\n",
      "fc3 - - weight - - Q0.15\n",
      "fc3 - - bias - - Q0.15\n"
     ]
    }
   ],
   "source": [
    "bit_depth = 16\n",
    "frac_bits_weight_list = []\n",
    "frac_bits_bias_list = []\n",
    "q_weight_list = []\n",
    "q_bias_list = []\n",
    "\n",
    "for i in range(layer_num):\n",
    "    weight, bias = model.get_layer(layer_names[i]).get_weights()\n",
    "    q_weight, f_weight, int_bits_weight, frac_bits_weight = quantize_array(weight, bit_depth=bit_depth)\n",
    "    q_bias, f_bias, int_bits_bias, frac_bits_bias = quantize_array(bias, bit_depth=bit_depth)\n",
    "    \n",
    "    q_weight_list.append(q_weight)\n",
    "    q_bias_list.append(q_bias)\n",
    "    frac_bits_weight_list.append(frac_bits_weight)\n",
    "    frac_bits_bias_list.append(frac_bits_bias)\n",
    "    print(\"**********\")\n",
    "    print(layer_names[i] + \" - - weight - - Q\"+str(int_bits_weight)+\".\"+str(frac_bits_weight))\n",
    "    print(layer_names[i] + \" - - bias - - Q\"+str(int_bits_bias)+\".\"+str(frac_bits_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate bias shifting and output shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 14 11 11 11 10 10  9  7  7  8]\n",
      "[16 17 15 15 16 15 16 17 15 14 14]\n"
     ]
    }
   ],
   "source": [
    "bias_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_bias_list)\n",
    "output_shift_list = np.array(frac_bits_weight_list) + np.array(frac_bits_activation_list[0:-1]) - np.array(frac_bits_activation_list[1:])\n",
    "print(bias_shift_list)\n",
    "print(output_shift_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the quantized weight and bias in 'weight.h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the weight tensor\n",
    "reordered_weight_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(q_weight_list[i].shape) == 4:  # CONV layer\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 2, 0))\n",
    "    else:  # FC layer\n",
    "        reordered_weight_list.append(np.moveaxis(q_weight_list[i], 1, 0))\n",
    "        \n",
    "# flatten the weight tensor\n",
    "reordered_weight_flatten_list = []\n",
    "for i in range(len(q_weight_list)):\n",
    "    if len(reordered_weight_list[i].shape) == 4: # CONV layer\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten('F'))\n",
    "    else: # FC layer\n",
    "        reordered_weight_flatten_list.append(reordered_weight_list[i].flatten())\n",
    "\n",
    "# save\n",
    "f = open(\"./weight.h\", \"w\")\n",
    "for i in range(len(q_weight_list)):\n",
    "    print(\"********\" + layer_names[i] + \"********\")\n",
    "    print(\"Bias Shift: \", bias_shift_list[i])\n",
    "    print(\"Output Shift:  \", output_shift_list[i])\n",
    "    print(\"Bias:  \", q_bias_list[i].astype(np.int16))\n",
    "    print(\"Weight:  \", reordered_weight_flatten_list[i].astype(np.int16))\n",
    "    \n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \"_WT {\" + str(reordered_weight_flatten_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS {\" + str(q_bias_list[i].astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_BIAS_LSHIFT \" + str(bias_shift_list[i]) + \"\\n\")\n",
    "    f.write(\"#define \" + layer_names[i].upper() + \"_OUT_RSHIFT \" + str(output_shift_list[i]) + \"\\n\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the quantized model in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model with output shifting\n",
    "q_model = QNet(bit=16, shift_list=output_shift_list)\n",
    "\n",
    "# set quantized weight and bias. (bias is shfited)\n",
    "for i in range(layer_num):\n",
    "    q_model.get_layer(layer_names[i]).set_weights([q_weight_list[i], q_bias_list[i]*(2**bias_shift_list[i])])\n",
    "\n",
    "# evaluation\n",
    "length = y.shape[0]\n",
    "positives = 0\n",
    "for i in range(length):\n",
    "    pred = q_model(np.round(x[i: i+1]*(2**frac_bits_activation_list[0])))  # quantize the input\n",
    "    pred = (pred > 0)\n",
    "    positives += np.sum(y[i]==pred)\n",
    "\n",
    "print(\"Accuracy after quantization: \", positives / length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate sample input and output, so we can verify the results in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = []\n",
    "intermediate_output.append(q_model.get_layer('input').output)\n",
    "\n",
    "for i in range(layer_num):\n",
    "    intermediate_output.append(q_model.get_layer(layer_names[i]).output)\n",
    "\n",
    "q_intermediate_layer_model = Model(inputs = q_model.input, outputs = intermediate_output)\n",
    "\n",
    "sample_output = q_intermediate_layer_model.predict(np.round(x[0: 1]*(2**frac_bits_activation_list[0])))\n",
    "\n",
    "# save\n",
    "f = open(\"./sample_input_output.h\", \"w\")\n",
    "temp = np.squeeze(sample_output[0], axis=0)\n",
    "temp = temp.transpose(2, 0, 1)\n",
    "temp = temp.flatten(\"F\")\n",
    "f.write(\"#define \" + \"INPUT_DATA {\" + str(temp.astype(np.int16).tolist())[1:-1] + \"}\\n\")\n",
    "\n",
    "for i in range(layer_num):\n",
    "    temp = np.squeeze(sample_output[i+1], axis=0)\n",
    "    print(temp.shape)\n",
    "    if len(temp.shape) == 3:\n",
    "        temp = temp.transpose(2, 0, 1)\n",
    "        temp = temp.flatten(\"F\")\n",
    "    \n",
    "    temp = temp.astype(np.int32) >> output_shift_list[i]\n",
    "    f.write(\"#define \" + layer_names[i].upper()+ \" {\" + str(temp.tolist())[1:-1] + \"}\\n\") \n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
